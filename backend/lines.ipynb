{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc41ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\askari tech\\appdata\\roaming\\python\\python311\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e7ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def camera_snap():\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    if not camera.isOpened():\n",
    "        print(\"Error: Camera not found or cannot be opened.\")\n",
    "        exit()\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture a frame.\")\n",
    "    else:\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        if _:\n",
    "            captured_image = np.array(buffer)\n",
    "    camera.release()\n",
    "    return captured_image\n",
    "\n",
    "def load_image():\n",
    "    image = input('please enter image path: ')\n",
    "    return cv2.imread(image),image\n",
    "\n",
    "\n",
    "def load_YOLO_model(weights_file):\n",
    "    # Load the YOLO model\n",
    "    model = YOLO(weights_file)\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect_objects(model, img_path):\n",
    "    # Run inference using the YOLO model\n",
    "    results = model(img_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_results_to_json(results):\n",
    "    results = results[0].tojson()\n",
    "    return json.loads(results)\n",
    "\n",
    "\n",
    "def detect_and_crop_face(img):\n",
    "    # Read the input image\n",
    "    original_image = img\n",
    "    \n",
    "    # Convert the image to grayscale for face detection\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize a face classifier using the Haar cascade classifier\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_classifier.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "    \n",
    "    # Initialize variables for face area and cropped face\n",
    "    face_area = 0\n",
    "    cropped_face = None\n",
    "    \n",
    "    # If faces are detected\n",
    "    if len(faces) > 0:\n",
    "        # Assuming the first detected face is the main one\n",
    "        x, y, w, h = faces[0]\n",
    "        \n",
    "        face_area = w * h\n",
    "        \n",
    "        # Crop the image to include only the detected face\n",
    "        cropped_face = original_image[y:y + h, x:x + w]\n",
    "    \n",
    "    return face_area, cropped_face\n",
    "\n",
    "\n",
    "def process_results(img, results, face_area, alpha=0.7, brightness_factor=1.2):\n",
    "    H, W, _ = img.shape\n",
    "    outline_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "    # Create an empty mask to accumulate all segmentations\n",
    "    combined_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    for result in results:\n",
    "        if result.masks is not None:  # Check if masks exist in the result\n",
    "            for j, mask in enumerate(result.masks.data):\n",
    "                mask = (mask.numpy() * 255).astype(np.uint8)  # Convert to np.uint8\n",
    "                mask = cv2.resize(mask, (W, H))\n",
    "\n",
    "                # Accumulate masks directly on combined_mask\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask)\n",
    "\n",
    "    # Find contours in the combined mask\n",
    "    contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw a boundary outline around each face contour\n",
    "    for contour in contours:\n",
    "        cv2.drawContours(img, [contour], 0, outline_color,2)\n",
    "\n",
    "    # Calculate the area of the segmented portion\n",
    "    area = np.sum(combined_mask / 255)\n",
    "    perc_area = ((area/face_area)*100)\n",
    "    if perc_area>0:\n",
    "        perc_area=perc_area+50\n",
    "\n",
    "    # Overlay the combined mask on the original image\n",
    "    segmented_img = cv2.bitwise_and(img, img, mask=combined_mask)\n",
    "\n",
    "    # Adjust transparency level\n",
    "    segmented_img = cv2.addWeighted(img, 1 - alpha, segmented_img, alpha, 0)\n",
    "\n",
    "    # Increase the brightness in the segmented area (make it brighter)\n",
    "    segmented_img = cv2.addWeighted(segmented_img, brightness_factor, np.zeros_like(segmented_img), 0, 0)\n",
    "\n",
    "    return segmented_img, perc_area\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3374e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter image path: cvb.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 -, 423.0ms\n",
      "Speed: 6.0ms preprocess, 423.0ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acne: 70.26540201879568%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 Acne Detect - v2 2023-02-09 10-16ams, 353.3ms\n",
      "Speed: 4.0ms preprocess, 353.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkcircles: 51.92240616660865%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 -s, 328.0ms\n",
      "Speed: 4.7ms preprocess, 328.0ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkspots: 68.15769484568976%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 800x800 (no detections), 384.8ms\n",
      "Speed: 6.0ms preprocess, 384.8ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oily: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 283.6ms\n",
      "Speed: 18.1ms preprocess, 283.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redness: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 800x800 2 acne-prone-skins, 336.4ms\n",
      "Speed: 8.9ms preprocess, 336.4ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texture: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 293.4ms\n",
      "Speed: 15.6ms preprocess, 293.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrinkles: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAMES = ['acne', 'darkcircles','darkspots', 'oily','redness', 'texture', 'wrinkles']\n",
    "weights = ['acne detection.pt', 'dark cirle final.pt', 'dark spots final.pt', 'oily.pt','redness 2.pt', 'texture.pt', 'wrinkle 3.pt']\n",
    "img,img_pth = load_image()\n",
    "#img = camera_snap()\n",
    "face_area,c_image = detect_and_crop_face(img)\n",
    "for idx, weight in enumerate(weights):\n",
    "    model = load_YOLO_model(weight)\n",
    "    results = detect_objects(model,c_image)\n",
    "    output, perc = process_results(c_image,results, face_area)\n",
    "    if MODEL_NAMES[idx]=='acne' and perc>0:\n",
    "        perc = perc +20\n",
    "    print(f\"{MODEL_NAMES[idx]}: {perc}%\")\n",
    "output_image_path = 'final_output_imageeee.png'\n",
    "cv2.imwrite(output_image_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "174ce266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter image path: cvb.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 -, 390.8ms\n",
      "Speed: 18.8ms preprocess, 390.8ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acne: 70.26540201879568%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 Acne Detect - v2 2023-02-09 10-16ams, 297.4ms\n",
      "Speed: 5.1ms preprocess, 297.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkcircles: 51.92393802935376%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 -, 288.7ms\n",
      "Speed: 8.3ms preprocess, 288.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 800x800 (no detections), 336.9ms\n",
      "Speed: 10.3ms preprocess, 336.9ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 640x640 (no detections), 308.8ms\n",
      "Speed: 3.6ms preprocess, 308.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 800x800 (no detections), 342.6ms\n",
      "Speed: 17.2ms preprocess, 342.6ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 640x640 (no detections), 313.8ms\n",
      "Speed: 5.1ms preprocess, 313.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAMES = ['darkspots', 'oily', 'redness', 'texture', 'wrinkles']\n",
    "weights = ['dark spots final.pt', 'oily.pt', 'redness 2.pt', 'texture.pt', 'wrinkle 3.pt']\n",
    "m1 = ['acne', 'darkcircles']\n",
    "w1 = ['acne detection.pt', 'dark cirle final.pt']\n",
    "img, img_pth = load_image()\n",
    "# img = camera_snap()\n",
    "face_area, c_image = detect_and_crop_face(img)\n",
    "output1 = c_image  # Initialize output1 with the cropped face image\n",
    "\n",
    "for idx, w in enumerate(w1):\n",
    "    model = load_YOLO_model(w)\n",
    "    results = detect_objects(model, c_image)\n",
    "    output1, perc = darkcircle_acne(c_image, results, face_area)\n",
    "    if m1[idx] == 'acne' and perc > 0:\n",
    "        perc = perc + 20\n",
    "    print(f\"{m1[idx]}: {perc}%\")\n",
    "\n",
    "# Initialize combined_mask and segmented_img with empty values\n",
    "combined_mask = np.zeros_like(c_image)\n",
    "segmented_img = np.zeros_like(c_image)\n",
    "\n",
    "for idx, weight in enumerate(weights):\n",
    "    model = load_YOLO_model(weight)\n",
    "    results = detect_objects(model, output1)\n",
    "    segmented, _ = segment_and_color_image(output1, results, face_area)\n",
    "\n",
    "    # Accumulate the segmented result\n",
    "    combined_mask = cv2.add(combined_mask, segmented)\n",
    "\n",
    "# Multiply the combined_mask by prominence_factor if needed\n",
    "prominence_factor = 2  # You can adjust this value\n",
    "combined_mask = cv2.multiply(combined_mask, np.where(combined_mask > 0, prominence_factor, 1).astype(c_image.dtype))\n",
    "\n",
    "# Overlay the combined_mask on the original face image\n",
    "output = cv2.addWeighted(c_image, 1, combined_mask, 1, 0)\n",
    "\n",
    "# Decrease the brightness of the output image (adjust alpha to control brightness)\n",
    "alpha = 0.7  # You can adjust this value\n",
    "darkened_output = cv2.addWeighted(output, 1 - alpha, output, 0, 0)\n",
    "\n",
    "output_image_path = 'final_output_imageeee.png'\n",
    "cv2.imwrite(output_image_path, darkened_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278e6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Input Image\", combined_output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e619220",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_output = c_image.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621f2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b10308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 -, 1 Hyperpigmentation, 336.1ms\n",
      "Speed: 3.0ms preprocess, 336.1ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "model_paths = [\n",
    "    'dark spots final.pt'\n",
    "]\n",
    "\n",
    "image_path = 'WhatsApp Image 2023-11-04 at 16.34.00.jpeg'\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "H, W, _ = img.shape\n",
    "\n",
    "# Create an empty mask to accumulate all segmentations\n",
    "combined_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "for model_path in model_paths:\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    results = model(img)\n",
    "\n",
    "    for result in results:\n",
    "        if result.masks is not None:  # Check if masks exist in the result\n",
    "            for j, mask in enumerate(result.masks.data):\n",
    "                mask = (mask.numpy() * 255).astype(np.uint8)  # Convert to np.uint8\n",
    "                mask = cv2.resize(mask, (W, H))\n",
    "\n",
    "                # Accumulate masks directly on combined_mask\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask)\n",
    "\n",
    "# Find contours in the combined mask\n",
    "contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw a boundary outline around each face contour\n",
    "for contour in contours:\n",
    "    cv2.drawContours(img, [contour], 0, (0, 255, 0), 2)\n",
    "\n",
    "# Overlay the combined mask on the original image\n",
    "segmented_img = cv2.bitwise_and(img, img, mask=combined_mask)\n",
    "\n",
    "# Adjust transparency level\n",
    "alpha = 0.7  # Adjust as needed\n",
    "segmented_img = cv2.addWeighted(img, 1 - alpha, segmented_img, alpha, 0)\n",
    "\n",
    "# Increase the brightness in the segmented area (make it brighter)\n",
    "brightness_factor = 1.2  # Adjust the factor to control brightness\n",
    "segmented_img = cv2.addWeighted(segmented_img, brightness_factor, np.zeros_like(segmented_img), 0, 0)\n",
    "\n",
    "# Save or display the segmented image\n",
    "cv2.imwrite('output_combined_3.png', segmented_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9dccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image():\n",
    "    image = input('please enter image path: ')\n",
    "    return cv2.imread(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f359814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb01732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
